$ bin/spark-submit --master yarn --num-executors 7 --executor-memory 6G --executor-cores 2 your-app.py 

当运行在YARN 上，Spark 应用使用固定的executor 数目，通过设置标志--num-executors 到spark-submit, spark-shell 等。
默认值是2，可以自己按需要增加。

也可以通过 --executor-memory 来设置每个executor 使用的内存。

通过 --executor-cores 向YARN 请求的申请的核数。

对于一组给定的硬件资源，Spark通常使用大量executor 中的少数（有多个核以及更多内存）会运行的更好，因为可以优化每个executor 之间的通信。然而注意有些集群对单个executor 的最大内存数有限制（默认8GB），不会让你启动更多。

Scala:

$ bin/spark-submit --master yarn --num-executors 7 --executor-memory 6G --executor-cores 2 --class com.your.package.yourclass /target/jar/path 